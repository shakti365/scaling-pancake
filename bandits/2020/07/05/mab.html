<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Multi Armed Bandits</h1><p class="page-description">Overview of Multi Armed Bandit techniques</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-05T00:00:00-05:00" itemprop="datePublished">
        Jul 5, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#bandits">bandits</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="introduction">Introduction:</h2>

<p>Multi Armed Bandit (MAB) algorithms can be applied in any type of decision making problem statement. A simple formulation of such problem statement is dynamic pricing of products.</p>

<ul>
  <li>A user comes on an e-commerce platform.</li>
  <li>The platform chooses to put a product on sale at certain price.</li>
  <li>User either makes a purchase at that price or doesn’t.</li>
  <li>Objective of the platform is to maximize the total profit from purchase of the product.</li>
</ul>

<p>In a MAB set-up, you can consider all the different price points for product as arms (or actions), let’s say there are $K$ of them. Every time a user comes on the platform, a new round is started, we observe till $T$ such rounds. In each round algorithm will select a price and show it to the user. In return, it observes a reward based on user’s behaviour. If the user makes a purchase, revenue becomes the reward. If the user doesn’t purchase, there is no reward. The algorithm can observe conversion only for the price the product was offered in each round. Therefore, it needs to try out different price to see which results in the maximum revenue. This presents us with a classical dilemma of <em>exploration - exploitation</em>.</p>

<p>With more trials and right balance between <em>exploration - exploitation</em>, the algorithm will converge to a solution that maximizes the revenue.</p>

<p><strong>Auxiliary Feedback:</strong> In addition to just the observed rewards from selected arms, there might be other feedback signals which could be of use. In the above problem, we know that the user would have converted if price was lower than the selected price in the round.</p>

<p><strong>Contexts:</strong> Select one action might not be optimal for any situation. A user who is likely to purchase can be offered the product at higher price compared to a user who would make a purchase only at lower prices. The price would also be dependent on the type of product as well. Including a context gives the algorithm a different high-level objective to do look at context before selecting the arm.</p>

<p><strong>Global Constraints:</strong> There might be conditions which need to be satisfied while selecting an arm. In our example, there could be an upper limit to the number of times users can convert at the lowest price (budget).</p>

<p><strong>Structured Actions:</strong> The arms could have some structure, not just selecting the correct price for one product but selecting prices for multiple products in a flash sale.</p>

<h2 id="stochastic-bandits">Stochastic Bandits:</h2>

<p><strong>Algorithm:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- for round t in total rounds T
	- pick arm a_t from set of arms A
	- observe reward r_t for the chosen arm a_t
</code></pre></div></div>

<p><strong>Regret:</strong> To understand if the algorithm’s performance over time we look at a metric called <em>Regret</em>. We can understand how the algorithm is performing at $t$ by comparing it with a policy which would have selected the best action for all the rounds.</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>μ</mi><mo>∗</mo></msup><mi mathvariant="normal">.</mi><mi>T</mi><mo>−</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><mi>μ</mi><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(T) = \mu^*.T - \sum_{t=1}^{T}{\mu(a_t)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.933136em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.738696em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mord">.</span><span class="mord mathdefault" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">μ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span>

<p>where,</p>

<p>​	$\mu(a_t)$ is the mean reward of selecting arm $a$</p>

<p>​	$\mu^* = \arg \max_{a}{\mu{(a)}}$ is the mean reward of the best performing arm</p>

<p>Since, $R(T)$ depends on randomness in algorithm and reward distribution we take expectation over it called <em>expected regret</em> $\mathbb{E}[R(T)]$</p>

<h3 id="uniform-exploration-first">Uniform Exploration First</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- first N rounds (exploration phase):
	- pick arm a_t from set of arms A in uniform random manner
	- observe reward r_t for the chosen arm a_t
- after N rounds (exploitation phase):
	- select arm a_t with highest average reward
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">TODO: regret analysis</code></p>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="shakti365/scaling-pancake"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/bandits/2020/07/05/mab.html" hidden></a>
</article>