<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Paper Notes: Asynchronous Advanatage Actor Critic (A3C)</h1><p class="page-description">Notes on A3C</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-25T00:00:00-05:00" itemprop="datePublished">
        Apr 25, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#rl">rl</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="asynchronous-methods-for-deep-reinforcement-learning">Asynchronous Methods for Deep Reinforcement Learning</h2>

<h3 id="contributions">Contributions:</h3>

<ul>
  <li>Use of asynchronous actors and parameter updated using Hogwild! strategy</li>
  <li>Parallel running learners can do more exploration and updates are less likely to be correlated</li>
</ul>

<h3 id="algorithm">Algorithm:</h3>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Policy: $\pi(a_t</td>
          <td>s_t;\theta)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Value function: $V(s_t;\theta_v)$</li>
  <li>Policy and value function share the same parameter except softmax output for policy and linear output for value function</li>
  <li>Adding entropy of the policy to the objective function is suggested to improve exploration; this later on becomes the soft actor-critic algorithm !?!</li>
  <li>Objective functions:
    <ul>
      <li>SGD with momentum</li>
      <li>RMSProp without shared statistics</li>
      <li>RMSProp with shared statistics</li>
    </ul>
  </li>
  <li>Algorithm in details:
    <ul>
      <li>Initialize global shared parameters: $\theta$ and $\theta_v$</li>
      <li>Initialize global shared counter: $T=0$</li>
      <li>Initialize thread step counter: $t \leftarrow 1$</li>
      <li>Repeat for every thread
        <ul>
          <li>Initialize gradients: $d\theta \leftarrow 0$ and $d\theta \leftarrow 0$</li>
          <li>Synchronize thread parameters with global parameters: $\theta^\prime = \theta$ and $\theta_v^\prime = \theta_v$</li>
          <li>t_start = t</li>
          <li>Get current state: $s_t$</li>
          <li>repeat:
            <ul>
              <li>
                <table>
                  <tbody>
                    <tr>
                      <td>Perform action from policy: $a_t = \pi(a_t</td>
                      <td>s_t;\theta)$</td>
                    </tr>
                  </tbody>
                </table>
              </li>
              <li>Store reward and next state: $r_t$ and $s_{t+1}$</li>
              <li>$t \leftarrow t + 1$</li>
            </ul>
          </li>
          <li>until terminal state</li>
          <li>Initialize $R=0$</li>
          <li>for $i$ in ${t-1,… t_{start}}$:
            <ul>
              <li>$R = r_i + \gamma R$</li>
              <li></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="shakti365/scaling-pancake"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/rl/2020/04/25/a3c.html" hidden></a>
</article>